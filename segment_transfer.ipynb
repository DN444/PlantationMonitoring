{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4187c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports, Drive mount, device and seeds\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Mount drive (Colab) - comment out if running locally\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except Exception as e:\n",
    "    # Not running in Colab or mount unavailable\n",
    "    pass\n",
    "\n",
    "# Device and reproducibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Use cuDNN fast autotuner when input sizes are constant (good for training)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load SAM and create predictor\n",
    "# Point the checkpoint path to your model checkpoint in Drive\n",
    "SAM_CHECKPOINT = \"/content/drive/MyDrive/plantation_data/models/sam_vit_b.pth\"\n",
    "if not os.path.exists(SAM_CHECKPOINT):\n",
    "    print(f\"Warning: checkpoint not found at {SAM_CHECKPOINT}. Update the path before running training.\")\n",
    "\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=SAM_CHECKPOINT)\n",
    "sam.to(device)\n",
    "# Put in train mode for fine-tuning\n",
    "sam.train()\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "print(\"SAM model and predictor ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset transform and ImageFolder preview (optional)\n",
    "class ConvertToRGB(object):\n",
    "    def __call__(self, img):\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ConvertToRGB(),\n",
    "    transforms.Resize(1024),\n",
    "    transforms.CenterCrop(1024),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "images_root = r\"/content/drive/MyDrive/plantation_data/train_dat\"\n",
    "if os.path.exists(images_root):\n",
    "    folder = datasets.ImageFolder(images_root, transform=transform)\n",
    "    print(\"Found classes:\", folder.classes)\n",
    "else:\n",
    "    print(\"images_root not found; skipping ImageFolder preview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Build image-mask pairs (robustly across subfolders)\n",
    "DATA_ROOT = r\"/content/drive/MyDrive/plantation_data\"\n",
    "IMAGES_DIR = os.path.join(DATA_ROOT, \"train_dat\")\n",
    "MASKS_DIR = os.path.join(DATA_ROOT, \"masks\")\n",
    "\n",
    "IMAGE_EXTS = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp')\n",
    "\n",
    "pairs = []\n",
    "if os.path.isdir(IMAGES_DIR) and os.path.isdir(MASKS_DIR):\n",
    "    img_subdirs = [d for d in sorted(os.listdir(IMAGES_DIR)) if os.path.isdir(os.path.join(IMAGES_DIR, d))]\n",
    "    mask_subdirs = [d for d in sorted(os.listdir(MASKS_DIR)) if os.path.isdir(os.path.join(MASKS_DIR, d))]\n",
    "    matched = sorted(set(img_subdirs).intersection(mask_subdirs))\n",
    "    print(f\"Matched subdirectories: {matched}\")\n",
    "\n",
    "    for sub in matched:\n",
    "        img_dir = os.path.join(IMAGES_DIR, sub)\n",
    "        mask_dir = os.path.join(MASKS_DIR, sub)\n",
    "        for img_file in sorted(os.listdir(img_dir)):\n",
    "            if not img_file.lower().endswith(IMAGE_EXTS):\n",
    "                continue\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            base = os.path.splitext(img_file)[0]\n",
    "            # find masks that either start with base or contain base\n",
    "            matched_masks = []\n",
    "            for root, _, files in os.walk(mask_dir):\n",
    "                for f in files:\n",
    "                    if not f.lower().endswith(IMAGE_EXTS):\n",
    "                        continue\n",
    "                    if f.startswith(base) or (base in f):\n",
    "                        matched_masks.append(os.path.join(root, f))\n",
    "            for m in sorted(matched_masks):\n",
    "                pairs.append({\"image\": img_path, \"mask\": m})\n",
    "\n",
    "print(f\"Total pairs found: {len(pairs)}\")\n",
    "\n",
    "# Sanity: split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "if len(pairs) > 1:\n",
    "    train_pairs, val_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    train_pairs, val_pairs = pairs, []\n",
    "print(f\"Train: {len(train_pairs)}, Val: {len(val_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395edd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Utility functions - padding and batch reader (produces pixel coords, int64 labels)\n",
    "def pad_to_square(image_tensor, size=1024):\n",
    "    # image_tensor shape: (B, C, H, W)\n",
    "    _, _, h, w = image_tensor.shape\n",
    "    pad_bottom = max(0, size - h)\n",
    "    pad_right = max(0, size - w)\n",
    "    if pad_bottom == 0 and pad_right == 0:\n",
    "        return image_tensor\n",
    "    padded = F.pad(image_tensor, (0, pad_right, 0, pad_bottom))\n",
    "    return padded\n",
    "\n",
    "def read_batch_random(data_pairs, predictor, visualize=False, target_size=1024, max_pos_points=10, neg_ratio=0.5):\n",
    "    # pick a random entry\n",
    "    ent = data_pairs[random.randint(0, len(data_pairs)-1)]\n",
    "    img_bgr = cv2.imread(ent['image'])\n",
    "    ann = cv2.imread(ent['mask'], cv2.IMREAD_GRAYSCALE)\n",
    "    if img_bgr is None or ann is None:\n",
    "        print('Could not read', ent)\n",
    "        return None, None, None\n",
    "    img = img_bgr[..., ::-1].copy()  # BGR->RGB\n",
    "\n",
    "    # apply predictor transform (this includes resizing to model expected input)\n",
    "    input_image = predictor.transform.apply_image(img)\n",
    "    input_tensor = torch.as_tensor(input_image).permute(2,0,1).unsqueeze(0).float()\n",
    "    # pad to square target_size if needed\n",
    "    input_tensor = pad_to_square(input_tensor, size=target_size)\n",
    "\n",
    "    # -- prepare GT mask: resize the annotation using nearest so labels remain integer\n",
    "    h_orig, w_orig = img.shape[:2]\n",
    "    scale = target_size / max(h_orig, w_orig)\n",
    "    new_w = int(round(w_orig * scale))\n",
    "    new_h = int(round(h_orig * scale))\n",
    "    mask_resized = cv2.resize(ann, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "    pad_h = target_size - mask_resized.shape[0]\n",
    "    pad_w = target_size - mask_resized.shape[1]\n",
    "    mask_padded = np.pad(mask_resized, ((0,pad_h),(0,pad_w)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Binarize mask (foreground > 0)\n",
    "    binary_mask = (mask_padded > 0).astype(np.uint8)\n",
    "\n",
    "    # Erode slightly to get interior points (optional)\n",
    "    eroded = cv2.erode(binary_mask, np.ones((5,5), np.uint8), iterations=1)\n",
    "    pos_coords = np.argwhere(eroded > 0)  # (y,x)\n",
    "    neg_coords = np.argwhere(binary_mask == 0)\n",
    "\n",
    "    points = []\n",
    "    labels = []\n",
    "    # positive points\n",
    "    if len(pos_coords) > 0:\n",
    "        n_pos = min(len(pos_coords), max_pos_points)\n",
    "        idxs = np.random.choice(len(pos_coords), n_pos, replace=False)\n",
    "        for i in idxs:\n",
    "            y,x = pos_coords[i]\n",
    "            points.append([int(x), int(y)])  # (x,y)\n",
    "            labels.append(1)\n",
    "    # negative points (random background) to teach the model boundaries\n",
    "    if len(neg_coords) > 0 and len(points) > 0:\n",
    "        n_neg = int(len(points) * neg_ratio)\n",
    "        n_neg = min(n_neg, len(neg_coords))\n",
    "        if n_neg > 0:\n",
    "            idxs = np.random.choice(len(neg_coords), n_neg, replace=False)\n",
    "            for i in idxs:\n",
    "                y,x = neg_coords[i]\n",
    "                points.append([int(x), int(y)])\n",
    "                labels.append(0)\n",
    "\n",
    "    if len(points) == 0:\n",
    "        return input_tensor, binary_mask, None, None\n",
    "\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,3,1); plt.imshow(img); plt.axis('off'); plt.title('Original')\n",
    "        plt.subplot(1,3,2); plt.imshow(binary_mask, cmap='gray'); plt.axis('off'); plt.title('GT binary')\n",
    "        plt.subplot(1,3,3); plt.imshow(binary_mask, cmap='gray');\n",
    "        for p,l in zip(points, labels):\n",
    "            c = 'r' if l==1 else 'b'\n",
    "            plt.scatter(p[0], p[1], s=80, c=c)\n",
    "        plt.axis('off'); plt.title('Points (red=pos, blue=neg)')\n",
    "        plt.show()\n",
    "\n",
    "    return input_tensor, binary_mask, np.array(points, dtype=np.float32), np.array(labels, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416086b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Freeze/unfreeze params and optimizer setup\n",
    "for p in sam.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze prompt_encoder and mask_decoder (and optionally image_encoder.neck if present)\n",
    "if hasattr(sam, 'prompt_encoder'):\n",
    "    for p in sam.prompt_encoder.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "if hasattr(sam, 'mask_decoder'):\n",
    "    for p in sam.mask_decoder.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "if hasattr(sam.image_encoder, 'neck'):\n",
    "    for p in sam.image_encoder.neck.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "trainable = [p for p in sam.parameters() if p.requires_grad]\n",
    "print(f\"Trainable parameter groups: {len(trainable)} tensors\")\n",
    "\n",
    "optimizer = optim.AdamW(trainable, lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.2)\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "# Losses\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def dice_loss(pred_probs: torch.Tensor, target: torch.Tensor, eps=1e-6):\n",
    "    # pred_probs and target are (B,H,W) with values in [0,1]\n",
    "    intersection = (pred_probs * target).sum(dim=[1,2])\n",
    "    union = pred_probs.sum(dim=[1,2]) + target.sum(dim=[1,2])\n",
    "    dice = (2.0 * intersection + eps) / (union + eps)\n",
    "    return 1.0 - dice.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training loop (with autocast, combined BCE+Dice, and negative sampling)\n",
    "sam.to(device)\n",
    "max_steps = 1000\n",
    "accumulation_steps = 4\n",
    "save_every = 500\n",
    "use_bce_dice = True\n",
    "dice_weight = 1.0\n",
    "\n",
    "for step in range(1, max_steps+1):\n",
    "    sample = read_batch_random(train_pairs, predictor, visualize=False, target_size=1024, max_pos_points=8, neg_ratio=0.5)\n",
    "    if sample is None:\n",
    "        continue\n",
    "    input_tensor, gt_mask_np, points, labels = sample\n",
    "    if input_tensor is None or gt_mask_np is None:\n",
    "        continue\n",
    "    if points is None or points.shape[0] == 0:\n",
    "        # skip samples without points\n",
    "        continue\n",
    "\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    gt_mask = torch.tensor((gt_mask_np > 0).astype(np.float32), device=device).unsqueeze(0)  # (1,H,W)\n",
    "\n",
    "    # Prepare prompt points and labels: SAM accepts pixel coords and per-point labels\n",
    "    input_points_t = torch.tensor(points, dtype=torch.float32, device=device).unsqueeze(0)  # (1,N,2)\n",
    "    input_labels_t = torch.tensor(labels, dtype=torch.int64, device=device).unsqueeze(0)    # (1,N)\n",
    "\n",
    "    # forward with autocast if cuda\n",
    "    use_autocast = torch.cuda.is_available()\n",
    "    autocast = torch.cuda.amp.autocast if use_autocast else torch.cpu.amp.autocast\n",
    "\n",
    "    with autocast():\n",
    "        image_embeddings = sam.image_encoder(input_tensor)\n",
    "        if len(image_embeddings.shape) == 3:\n",
    "            image_embeddings = image_embeddings.unsqueeze(0)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "            points=(input_points_t, input_labels_t),\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "\n",
    "        image_pe = sam.prompt_encoder.get_dense_pe()\n",
    "\n",
    "        low_res_masks, iou_preds = sam.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=image_pe,\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "\n",
    "        # pick best mask per batch element using iou_preds\n",
    "        best_idx = iou_preds.argmax(dim=1)  # (B,)\n",
    "        B = low_res_masks.shape[0]\n",
    "        chosen = low_res_masks[torch.arange(B, device=device), best_idx]  # (B,H',W')\n",
    "\n",
    "        # upsample logits to original input size\n",
    "        prd_logits_resized = torch.nn.functional.interpolate(\n",
    "            chosen.unsqueeze(1), size=input_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "        ).squeeze(1)  # (B,H,W)\n",
    "\n",
    "        # BCE on logits\n",
    "        bce = bce_loss(prd_logits_resized, gt_mask)\n",
    "        if use_bce_dice:\n",
    "            prd_probs = torch.sigmoid(prd_logits_resized)\n",
    "            dloss = dice_loss(prd_probs, gt_mask)\n",
    "            seg_loss = bce + dice_weight * dloss\n",
    "        else:\n",
    "            seg_loss = bce\n",
    "\n",
    "    loss = seg_loss / accumulation_steps\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(trainable, max_norm=1.0)\n",
    "\n",
    "    if step % accumulation_steps == 0:\n",
    "        if scaler is not None:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    # compute IoU for logging (no grad)\n",
    "    with torch.no_grad():\n",
    "        prd = (torch.sigmoid(prd_logits_resized) > 0.5).float()\n",
    "        inter = (gt_mask * prd).sum()\n",
    "        union = gt_mask.sum() + prd.sum() - inter\n",
    "        iou = inter / (union + 1e-6)\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: loss={seg_loss.item():.6f} iou={iou.item():.6f}\")\n",
    "\n",
    "    if step % save_every == 0:\n",
    "        out_path = \"/content/drive/MyDrive/plantation_data/models/fine_tuned_sam_vit_b_step_{}.pth\".format(step)\n",
    "        torch.save(sam.state_dict(), out_path)\n",
    "        print(\"Saved checkpoint to\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae02742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Quick evaluation helper (visualize prediction on a val sample)\n",
    "sam.eval()\n",
    "with torch.no_grad():\n",
    "    if len(val_pairs) > 0:\n",
    "        sample = read_batch_random(val_pairs, predictor, visualize=False, target_size=1024, max_pos_points=8, neg_ratio=0.5)\n",
    "        if sample is None:\n",
    "            print('Val sample read failed')\n",
    "        else:\n",
    "            input_tensor, gt_mask_np, points, labels = sample\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            image_embeddings = sam.image_encoder(input_tensor)\n",
    "            if len(image_embeddings.shape) == 3:\n",
    "                image_embeddings = image_embeddings.unsqueeze(0)\n",
    "            if points is None or points.shape[0] == 0:\n",
    "                print('No points available in this val sample')\n",
    "            else:\n",
    "                pts = torch.tensor(points, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                labs = torch.tensor(labels, dtype=torch.int64, device=device).unsqueeze(0)\n",
    "                sparse_embeddings, dense_embeddings = sam.prompt_encoder(points=(pts, labs), boxes=None, masks=None)\n",
    "                image_pe = sam.prompt_encoder.get_dense_pe()\n",
    "                low_res_masks, iou_preds = sam.mask_decoder(\n",
    "                    image_embeddings=image_embeddings,\n",
    "                    image_pe=image_pe,\n",
    "                    sparse_prompt_embeddings=sparse_embeddings,\n",
    "                    dense_prompt_embeddings=dense_embeddings,\n",
    "                    multimask_output=True,\n",
    "                )\n",
    "                best_idx = iou_preds.argmax(dim=1)\n",
    "                chosen = low_res_masks[torch.arange(low_res_masks.shape[0], device=device), best_idx]\n",
    "                prd_logits_resized = torch.nn.functional.interpolate(chosen.unsqueeze(1), size=input_tensor.shape[2:], mode='bilinear', align_corners=False).squeeze(1)\n",
    "                prd = (torch.sigmoid(prd_logits_resized) > 0.5).cpu().numpy()[0]\n",
    "                plt.figure(figsize=(10,5))\n",
    "                plt.subplot(1,2,1); plt.imshow(gt_mask_np, cmap='gray'); plt.title('GT'); plt.axis('off')\n",
    "                plt.subplot(1,2,2); plt.imshow(prd, cmap='gray'); plt.title('Pred'); plt.axis('off')\n",
    "                plt.show()\n",
    "    else:\n",
    "        print('No val samples to visualize')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6928ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save final model\n",
    "final_out = \"/content/drive/MyDrive/plantation_data/models/fine_tuned_sam_vit_b_final.pth\"\n",
    "try:\n",
    "    torch.save(sam.state_dict(), final_out)\n",
    "    print('Saved final model to', final_out)\n",
    "except Exception as e:\n",
    "    print('Saving final model failed:', e)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
